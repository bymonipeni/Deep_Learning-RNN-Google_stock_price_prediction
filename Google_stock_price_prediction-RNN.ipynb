{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "dataset_train = pd.read_csv('/Users/monipeni/Documents/Deep_Learning_A_Z/Volume 1 - Supervised Deep Learning/Building a RNN/Google_Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close      Volume\n",
       "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape # it contains 1258 stock prices between 2012 and 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "\n",
    "# this will be the training set on which our RNN will be trained. \n",
    "# [ :, 1:2] We want the second column of our data. We put : because we want all the rows. \n",
    "# The second column is index 1. But we can not input the index 1 only, because we want to create a Numpy array and \n",
    "# not a simple vector. That´s why we write 1:2, column one to two, but two is excluded so only take the first column\n",
    "# .values create the Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape # training_set is a Numpy array of 1258 lines corresponding to the 1258 stock prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "# Whenever we build a RNN and especially if there is a sigmoid function as the activation function in the output\n",
    "# layer of our recurrent neural network, is recommended to apply normalisation. And that´s MinMaxScaler.\n",
    "# feature_range = (0,1) is per default. All the stock prices are gonna be numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output. \n",
    "\n",
    "# 60 timesteps correspond to the 60 previous financial days. There are 20 financial days in one month, so that means \n",
    "# each day we are gonna look at the 3 previous month to try to predict the stock price the next day.\n",
    "\n",
    "X_train = [] # me make an empty list, later we will append the prices \n",
    "y_train = [] # same here\n",
    "for i in range(60, 1258): # the first index we are gonna train is the number 60, because we need 60 previous timesteps.\n",
    "                        # python starts with index 0, that´s why we don´t write 61.\n",
    "                        # the index of the last price is 1257, but we write 1258 because in python the index rest one.\n",
    "    X_train.append(training_set_scaled[i-60:i, 0]) # i-60:i  for each price we want to predict, we look 60 prices before.\n",
    "                                                # is gonna look the rows from 0 to 59 ( are 60 lines) to predict the 60th\n",
    "                                                # 0 is the column, is only one column and then is index 0\n",
    "    y_train.append(training_set_scaled[i, 0]) # i is the price we want to predict\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "\n",
    "# add a new dimension in our Numpy Array \n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# the first argument is the numpy array that we want to reshape, add this new dimension corresponding to the predictor,\n",
    "# or in our problem, the indicator\n",
    "# in the 2nd argument we need to specify this new structure, that is this new shape we want our Numpy array to have,\n",
    "# a 3D array, containing the following 3 dimensions: first, the batch size, which will correspond to the total number\n",
    "# of observations we have. Second, is timesteps. And the third step is the one corresponding to the indicators,\n",
    "# the predictors. \n",
    "# Example: we have to predict stock prices from Apple, we know iphone uses many material from Samsung. So the \n",
    "# stock prices from Apple and Samsung might be highly correlated. If we add as a third dimension the stock prices \n",
    "# from Samsung, the prediction will be higher.\n",
    "# X_train.shape[0] gives me the number of lines of X_train\n",
    "# X_train.shape[1] gives me the number of columns of X_train, which is exactly the number of timesteps\n",
    "# 1 because we have only one indicator, we don´t add any other data to make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # we have 3 dimensions now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential() # as a sequence of layers. We called regressor because this time we are predicting a \n",
    "                         # continuous value. Classification is about predicting a cathegory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "\n",
    "\n",
    "# LSTM arguments: \n",
    "# first : number of LSTM units or neurons you want to have in this LSTM layer. You can choose any number of neurons,\n",
    "# but we choose 50 because we want a model with high dimensionality. If it´s only 5 neurons, will be too little.\n",
    "# second : return_sequences we put True, because we want to add another layer. When we don´t want to add more layers,\n",
    "# we can put False or don´t write it, because False is the default value.\n",
    "# third : input_shape is the shape of the input containing X_train that we created in 3D. But we only have to indicate\n",
    "# the last 2 arguments of the 3D, the timesteps and the indicators, because the first one, corresponding to the\n",
    "# observations will be automatically taken into account\n",
    "\n",
    "regressor.add(Dropout(0.2)) # to avoid over fitting, drop 20% of the neurons is the recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True)) # we don´t need to write the input layer anymore, because \n",
    "                                                        # the model knows from before\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50)) # we are not gonna add more LSTM layers, so we delete the return_sequences = True\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1)) # the output layer is fully connected to the fourth LSTM layer, that´s why we use\n",
    "                                # DEnse, and it only has one argument, the number of neurons that needs to be in this\n",
    "                                # output layer. We are predicting a real value, corresponding to the stock price, so\n",
    "                                # the output has only one dimension (units=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error') \n",
    "\n",
    "# the most relevant optimizers in RNN are 'adam' or 'RMS prop', in this case we use 'adam', because is very \n",
    "# powerful and it always perform some relevant updates of the weights.\n",
    "# loss = we are dealing with a regression problem because we have to predict a continuous value, so the loss is \n",
    "# the mean squared error ( the error can be mesure this time by the mean of the squared differences between the\n",
    "# predictions and the targets or real values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 31s 26ms/step - loss: 0.0381\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0063\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0055\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0053\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0051\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0050\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0049\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0045\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0044\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0041\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0042\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0040\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0037\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 857s 716ms/step - loss: 0.0043\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 25s 21ms/step - loss: 0.0039\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0039\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0034\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0039\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0040\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0039\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0035\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 5741s 5s/step - loss: 0.0034\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0037\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0033\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 29s 24ms/step - loss: 0.0033\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0031\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 28s 23ms/step - loss: 0.0035\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0033\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0028\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0031\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0030\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0029\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0030\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0026\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0024\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0025\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0027\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0033\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0027\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0028\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0027\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0025\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 23s 20ms/step - loss: 0.0025\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0023\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0023\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0022\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0024\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0022\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0023\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0025\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0025\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0021\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0021\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0019\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0022\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0020\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0022\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0021\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0018\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0017\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0017\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0019\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0018\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0019\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0017\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0016\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0016\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0018\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0019\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0016\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0016\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0015\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0016\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0018\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0016\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0015\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0014\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0015\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0016\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x115485550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "\n",
    "# fit has 4 arguments:\n",
    "# 1. Input \n",
    "# 2. Ground truth\n",
    "# 3. Number of epochs : how many iterations do you want your RNN to be trained.\n",
    "# 4. Batch_size : numbers of samples processed before the model is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the real stock price of 2017\n",
    "\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv') # this data set contains 20 stock prices of January 2017\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values # [:,1:2]we want all the rows and the second column of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) \n",
    "\n",
    "# 'Open' is the name of the column in our dataset that contains the Google stock prices\n",
    "# axis = 0 , because we want to concatenate the lines, we want to add the stock prices of the test set to the\n",
    "# stock prices of the training set, so we need to make a concatenation around the vertical axis, which is 0.\n",
    "# for a horizontal concatenation we use axis = 1\n",
    "\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values \n",
    "\n",
    "# to the len of the total dataset (final index of the whole dataset) we substract the len of the dataset_test (that\n",
    "# is exactly 20 )( we have 20 prices for every month, from monday to friday every week is 5x4=20) \n",
    "# and we obtain January 3rd, 2017\n",
    "# - 60 because we need the 60 previous prices to predict\n",
    "# .values to make this a Numpy Array\n",
    "\n",
    "inputs = inputs.reshape(-1,1) # we create the 3D structure, always with arguments -1,1\n",
    "inputs = sc.transform(inputs) # we scale our inputs, but has to be with the same scaling that was applied to \n",
    "                            # the training set , we don´t use fit , we used transform method\n",
    "    \n",
    "X_test = [] # we copy and paste the data structure that we did before. We don´t need y_test because this time \n",
    "            #we are not doing some training, we are doing some predictions.\n",
    "for i in range(60, 80): # before we used the numbers 60,1258. We still start at 60 because we need the 60 previous \n",
    "                        # predictions, but this time we don´t need the whole dataset, we don´t need all the stock\n",
    "                        # prices of 5 years, we only need to get our input for the test set and the test set \n",
    "                        # contains only 20 financial days (each of the stock prices of January 2017), so 60+20 = 80\n",
    "    X_test.append(inputs[i-60:i, 0]) # we keep the 0 which correspond to the Open column, the stocks prices\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) # we add the new dimension for 3D\n",
    "\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) # we need to invert the scaling of our \n",
    "                    # predictions, because our regressor was trained to predict the scaled values of the stock\n",
    "                    # prices. To get the original scale of these scaled predicted values we need to apply the\n",
    "                    # inverse transform method from our scaling sc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the real Google stock price in red and our predicted Google stock price in blue. We get this \n",
    "comparison of the real and the predicted prices for the whole month of January 2017 and we obtained good results.\n",
    "In the parts of the predictions containing some spikes, our predictions lag behind the actual values because our\n",
    "model can´t react to fast nonlinear changes. For the parts of the predictions containing smooth changes our model\n",
    "reacts pretty well and manages to follow the upward and downward trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Regression, the way to evaluate the model performance is with a metric called RMSE (Root Mean Squared Error). It is calculated as the root of the mean of the squared differences between the predictions and the real values.\n",
    "\n",
    "However for our specific Stock Price Prediction problem, evaluating the model with the RMSE does not make much sense, since we are more interested in the directions taken by our predictions, rather than the closeness of their values to the real stock price. We want to check if our predictions follow the same directions as the real stock price and we don’t really care whether our predictions are close the real stock price. The predictions could indeed be close but often taking the opposite direction from the real stock price.\n",
    "\n",
    "Nevertheless if we are interested in the code that computes the RMSE for our Stock Price Prediction problem, it would be:\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = math.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Google Stock Price values of January 2017 (that is around 800) to get a relative error, as opposed to an absolute error. It is more relevant since for example if you get an RMSE of 50, then this error would be very big if the stock price values ranged around 100, but it would be very small if the stock price values ranged around 10000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different ways to improve the RNN model:\n",
    "\n",
    "-Getting more training data: we trained our model on the past 5 years of the Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "-Increasing the number of timesteps: the model remembered the stock prices from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 timesteps (3 months). We could try to increase the number of timesteps, by choosing for example 120 timesteps (6 months).\n",
    "\n",
    "-Adding some other indicators: if we have the financial instinct that the stock price of some other companies might be correlated to the one of Google, we could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "-Adding more LSTM layers: we built a RNN with four LSTM layers but we could try with even more.\n",
    "\n",
    "-Adding more neurones in the LSTM layers: we highlighted the fact that we needed a high number of neurones in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurones in each of our 4 LSTM layers. We could try an architecture with even more neurones in each of the 4 (or more) LSTM layers.\n",
    "\n",
    "\n",
    "We can do some Parameter Tuning on the RNN model we implemented. This time we are dealing with a Regression problem because we predict a continuous outcome (the Google Stock Price), so we use scoring = 'neg_mean_squared_error' \n",
    "in the GridSearchCV class parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
